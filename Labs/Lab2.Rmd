---
title: "Lab 2"
author: "Roxanne Li"
date: "2024-01-18"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
#| message: false
library(opendatatoronto)
library(tidyverse)
library(stringr)
library(skimr) # EDA
library(visdat) # EDA
library(janitor)
library(lubridate)
library(ggrepel)
```

## Q1

Downloading the data on TTC subway delays in 2022. 
```{r}
res <- list_package_resources("996cfe8d-fb35-40ce-b569-698d51fc683b") # obtained code from searching data frame above
res <- res |> mutate(year = str_extract(name, "202.?"))
delay_2022_ids <- res |> filter(year==2022) |> select(id) |> pull()

delay_2022 <- get_resource(delay_2022_ids)

# make the column names nicer to work with
delay_2022 <- clean_names(delay_2022)
delay_codes <- get_resource("3900e649-f31e-4b79-9f20-4731bbfd94f7")
```

Preprocessing the data according to **2_eda_dataviz_additions.qmd**:

```{r}
## remove duplicates
delay_2022 <- delay_2022 |> distinct()

## cleaning up the YU/BD line
delay_2022 <- delay_2022 |> 
  mutate(contains_yu_bd = str_detect(str_to_lower(line), "bd")&str_detect(str_to_lower(line), "yu")) |> 
  mutate(line = ifelse(contains_yu_bd, ifelse(line=="YU/BD", line, "YU/BD"), line))  |> 
  select(-contains_yu_bd) 

## removing non-standardized lines
delay_2022 <- delay_2022 |> filter(line %in% c("BD", "YU", "SHP", "SRT", "YU/BD"))

## left-joining delay codes
delay_2022 <- delay_2022 |> 
  left_join(delay_codes |> rename(code = `SUB RMENU CODE`, code_desc = `CODE DESCRIPTION...3`) |> select(code, code_desc)) 

delay_2022 <- delay_2022 |>
  mutate(code_srt = ifelse(line=="SRT", code, "NA")) |> 
  left_join(delay_codes |> rename(code_srt = `SRT RMENU CODE`, code_desc_srt = `CODE DESCRIPTION...7`) |> select(code_srt, code_desc_srt))  |> 
  mutate(code = ifelse(code_srt=="NA", code, code_srt),
         code_desc = ifelse(is.na(code_desc_srt), code_desc, code_desc_srt)) |> 
  select(-code_srt, -code_desc_srt)

## cleaning up station names
delay_2022 <- delay_2022 |> 
  mutate(station_clean = ifelse(str_starts(station, "ST"), word(station, 1,2), word(station, 1)))

head(delay_2022)
```

Using the `delay_2022` data, plot the five stations with the highest mean delays. Facet the graph by `line`.

```{r}
delay_2022 |> 
  group_by(line, station_clean) |>
  summarise(mean_delay = mean(min_delay)) |>
  arrange(line, desc(mean_delay))|>
  slice(1:5) |>
  ggplot(aes(x = station_clean, y = mean_delay)) +
    geom_col() +
    facet_wrap(vars(line), scales = "free_y") +
    coord_flip()
```

## Q2
Restrict the `delay_2022` to delays that are greater than 0 and to only have delay reasons that appear in the top 50% of most frequent delay reasons. Perform a regression to study the association between delay minutes, and two covariates: line and delay reason. It's up to you how to specify the model, but make sure it's appropriate to the data types. Comment briefly on the results, including whether results generally agree with the exploratory data analysis above. 

Preparing the data:
```{r}
# get the top 50% of most frequent delay reasons (exclude NAs)
most_frequent_delay_reasons <- delay_2022 |>
  filter(!is.na(code_desc), min_delay>0) |>
  group_by(code_desc) |>
  summarise(n_obs = n()) |>
  arrange(-n_obs) |>
  slice(1:(n() / 2))

# filter data based on the most_frequent_delay_reasons
data <- delay_2022 |>
  filter(min_delay>0 & code_desc %in% most_frequent_delay_reasons$code_desc)

# shorten the code names
data <- data |> 
  mutate(code_red = case_when(
    str_starts(code_desc, "No") ~ word(code_desc, 1, 2),
    str_starts(code_desc, "Operator") ~ word(code_desc, 1,2),
    TRUE ~ word(code_desc,1))
         )
```

Using PCA to create the principal components that represent information about the delay reasons:
```{r}
dwide <- data |> 
  group_by(line, station_clean, code_red) |> 
  summarise(n_delay = n()) |> 
  pivot_wider(names_from = code_red, values_from = n_delay) |> 
  mutate(
    across(everything(), ~ replace_na(.x, 0))
  )

delay_pca <- prcomp(dwide[,3:ncol(dwide)])

df_out <- as_tibble(delay_pca$x)
df_out <- bind_cols(dwide |> select(line, station_clean), df_out)
PCA_data <- df_out[, 1:4]
```

Merging two principal components representing the delay reasons with the original data:
```{r}
merged_df <- merge(data, PCA_data, by = c("line", "station_clean"))
```

Fitting a linear regression model to the line variable as a factor and the two principal components of delay reasons:
$$
Y_{i} = \beta_0 + \beta_{1}Z_{i} + \beta_{2}X_{1i} + \beta_{3}X_{2i}
$$
where $Y_{i}$ is the delay minutes, $Z_{i}$ is the factor variable of line, $X_{1i}$ and $X_{2i}$ are the principal components.

```{r}
merged_df$line <- as.factor(merged_df$line)

model <- lm(min_delay ~ line + PC1 + PC2, data=merged_df)
summary(model)
```
The model did not fit well as the adjusted R-squared value is very low. However, the results show that the two principal components containing information about delay reasons are significant. Besides, the variable for line SRT is significant and it agrees with the EDA that the SRT line in general has longer delays.

## Q3
Using the `opendatatoronto` package, download the data on mayoral campaign contributions for 2014 and clean it up. Hints:
    + find the ID code you need for the package you need by searching for 'campaign' in the `all_data` tibble above
    + you will then need to `list_package_resources` to get ID for the data file
    + note: the 2014 file you will get from `get_resource` has a bunch of different campaign contributions, so just keep the data that relates to the Mayor election
    + clean up the data format (fixing the parsing issue and standardizing the column names using `janitor`)

```{r}
list_package_resources("e869d365-2c15-4893-ad2a-744ca867be3b") # obtained code from searching data frame
```

Obtaining the mayor contribution data:

```{r}
camp_2014 <- get_resource("8b42906f-c894-4e93-a98e-acac200f34a4")
mayor <- camp_2014$"2_Mayor_Contributions_2014_election.xls"
colnames(mayor) <- as.character(mayor[1, ])
mayor <- mayor[-1, ]
mayor <- clean_names(mayor)
head(mayor)
```

## Q4

Summarize the variables in the dataset. Are there missing values, and if so, should we be worried about them? Is every variable in the format it should be? If not, create new variable(s) that are in the right format.

The table below shows the number and percentage of missing values in each column.

```{r}
skim(mayor)
```
```{r}
vis_dat(mayor)
```
There are 6 columns in the dataset that have almost all the values missing, which is worrisome if we want to analyze for example, the address of the contributors and the relationship to candidate. Not all columns are in the correct data type.

```{r}
mayor$contribution_amount <- as.numeric(mayor$contribution_amount)
```


## Q5

Visually explore the distribution of values of the contributions. What contributions are notable outliers? Do they share a similar characteristic(s)? It may be useful to plot the distribution of contributions without these outliers to get a better sense of the majority of the data. 

```{r}
mayor |>
  ggplot(aes(x = contribution_amount)) + 
   geom_histogram()
```
```{r}
mayor |>
  filter(contribution_amount > 1e+05)
```
The contribution by Doug Ford is an outlier so we exclude it from the data and plot the histogram again.

```{r}
mayor |>
  filter(contribution_amount <= 1e+05) |>
  ggplot(aes(x = contribution_amount)) + 
   geom_histogram()
```

There are still outliers, so we try to set the threshold at 4000.

```{r}
mayor |>
  filter(contribution_amount > 4000)
```

These outliers have the same characteristic that the monetary contribution comes from the mayor candidate himself/herself.

We exclude any amounts above 4000 and plot the histogram again:

```{r}
mayor |>
  filter(contribution_amount <= 4000) |>
  ggplot(aes(x = contribution_amount)) + 
   geom_histogram()
```

Excluding outliers with contribution amount above 4000, the majority of the contribution amounts are below 1000.

# Q6

List the top five candidates in each of these categories:
    + total contributions
    + mean contribution
    + number of contributions
    
Top five candidates by total contributions:
```{r}
mayor |>
  group_by(candidate) |>
  summarise(total_contribution = sum(contribution_amount)) |>
  arrange(-total_contribution) |>
  slice(1:5)
```
Top five candidates by mean contribution:

```{r}
mayor |>
  group_by(candidate) |>
  summarise(mean_contribution = sum(contribution_amount) / n()) |>
  arrange(-mean_contribution) |>
  slice(1:5)
```
Top five candidates by number of contributions:

```{r}
mayor |>
  group_by(candidate) |>
  summarise(n_contribution = n()) |>
  arrange(-n_contribution) |>
  slice(1:5)
```

## Q7

Repeat 6 but without contributions from the candidates themselves.

Top five candidates by total contributions:
```{r}
mayor |>
  mutate(self_contribute = ifelse(contributors_name==candidate, 1, 0)) |>
  filter(self_contribute != 1) |> 
  group_by(candidate) |>
  summarise(total_contribution = sum(contribution_amount)) |>
  arrange(-total_contribution) |>
  slice(1:5)
```
Top five candidates by mean contribution:

```{r}
mayor |>
  mutate(self_contribute = ifelse(contributors_name==candidate, 1, 0)) |>
  filter(self_contribute != 1) |>
  group_by(candidate) |>
  summarise(mean_contribution = sum(contribution_amount) / n()) |>
  arrange(-mean_contribution) |>
  slice(1:5)
```

Top five candidates by number of contributions:

```{r}
mayor |>
  mutate(self_contribute = ifelse(contributors_name==candidate, 1, 0)) |>
  filter(self_contribute != 1) |>
  group_by(candidate) |>
  summarise(n_contribution = n()) |>
  arrange(-n_contribution) |>
  slice(1:5)
```
## Q8

How many contributors gave money to more than one candidate?  

```{r}
mutiple_cands <- mayor |>
  group_by(contributors_name) |>
  summarise(n_candidate = length(unique(candidate))) |>
  filter(n_candidate > 1) 
nrow(mutiple_cands)
```

184 contributors gave money to more than one candidate.

